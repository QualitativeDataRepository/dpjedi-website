## Computational Reproducibility {#computational-reproducibility}


### Open code {#open-code}

Mandating or encouraging authors to share both data and code alongside their manuscripts has all the above benefits of sharing either of these separately, but also means that reviewers (and later, readers) can check the computational reproducibility of the results (i.e., does running the code on the data produce the same results that are reported in the paper).

The [American Economic Association](https://www.aeaweb.org/){:target="_blank"} provides helpful [guidance on implementation of their data and code availability policy](https://aeadataeditor.github.io/aea-de-guidance/){:target="_blank"} that could easily be applied to other journals and fields. [The Berkeley Institute for Transparency in the Social Sciences](https://www.bitss.org/){:target="_blank"} offers a handy [guide on conducting reproducibility checks](https://bitss.github.io/ACRE/){:target="_blank"}.

For a discussion of the impact of journal data policy strictness on the code re-execution rate (i.e., how likely the code is to run without errors) and a set of recommendations for code dissemination aimed at journals, see [Trisovic et al. (2020)](https://arxiv.org/abs/2103.12793){:target="_blank"}.

### Pre-publication verification of analyses {#pre-publication-verification-of-analyses}

Some journals have now adopted a policy whereby data and code are not only required for publication in the journal, but must be checked before publication to ensure that the analyses are reproducible – that the results in the manuscript match the results that are produced when someone who is not one of the authors re-runs the code on the data. This is called pre-publication “verification of analyses”, “data and code replication”, or “reproduction of analyses”. See [Willis & Stodden (2020)](https://doi.org/10.1162/99608f92.25982dcf){:target="_blank"} for a useful overview of how to leverage policies, workflows, and infrastructure to ensure computational reproducibility in publication.

For more information on how to implement a policy like this at your journal, see the [Data and Code Guidance by Data Editors](https://social-science-data-editors.github.io/guidance/){:target="_blank"} developed by Lars Vilhuber and colleagues, which is used by the [American Economic Association journals](https://www.aeaweb.org/journals){:target="_blank"}, [Canadian Journal of Economics](https://onlinelibrary.wiley.com/journal/15405982){:target="_blank"}, the [Review of Economic Studies](https://academic.oup.com/restud){:target="_blank"}, and the [Economic Journal](https://academic.oup.com/ej){:target="_blank"} as a reference. This [practical guide on training students to assess the reproducibility of articles in the social sciences](https://labordynamicsinstitute.github.io/ldilab-manual/index.html){:target="_blank"} may also be of interest.

Several journals in political science also require pre-publication verification. See for example [State Politics & Policy Quarterly](https://www.cambridge.org/core/journals/state-politics-and-policy-quarterly/information/instructions-contributors){:target="_blank"}, [Political Analysis](https://www.cambridge.org/core/journals/political-analysis/information/instructions-contributors){:target="_blank"}, and the [American Journal of Political Science](https://ajps.org/){:target="_blank"}.

### Increasing Computational Reproducibility {#increasing-computational-reproducibility}

[Willis and Stodden (2020)](https://hdsr.mitpress.mit.edu/pub/f0obb31j/release/1?readingCollection=c6cf45bb){:target="_blank"} highlight nine decision points for journals looking to improve the quality and rigor of computational research and suggest that journals reporting computational research aim to include “assessable reproducible research artifacts” along with published articles to.

The [*American Journal of Political Science* Verification Policy](https://ajps.org/ajps-verification-policy/){:target="_blank"} provides a role-model example of how computational research can be made more rigorous and error-free through additional steps in the editorial process – but it also shows how this requires resources on top of the procedures editors have become accustomed to over decades.

### Verification Reports {#verification-reports}

Verification Reports (VRs) are an article format focusing specifically on computational reproducibility and analytic robustness. VRs meet this objective by repeating the original analyses or reporting new analyses of original data. In doing so they provide the verifiers conducting the investigation with professional credit for evaluating one of the most fundamental forms of credibility: whether the claims in previous studies are justified by their own data. Chris Chambers has introduced this format at [*Cortex*](https://www.journals.elsevier.com/cortex){:target="_blank"} (see his introductory [editorial](https://doi.org/10.1016/j.cortex.2020.04.020){:target="_blank"} and [materials for journals](https://osf.io/9g5d8/){:target="_blank"}). For examples of the first two VRs published by *Cortex*, see [Chalkia et al. (2020)](https://doi.org/10.1016/j.cortex.2020.03.031){:target="_blank"} and [Mirman et al. (2021)](https://doi.org/10.1016/j.cortex.2021.01.017){:target="_blank"}. If you’re interested in including VRs as an article type at your journal, Cortex’s [author guidelines](https://www.elsevier.com/__data/promis_misc/VR_GuideForAuthors.pdf){:target="_blank"} provide more information on this format.


### Resources for open research workflows {#resources-for-open-workflows}

Many platforms can aid researchers in making their entire workflows open and transparent. Here are a few examples:

- [Code Ocean](http://codeocean.com/){:target="_blank"}
- [The Whole Tale](http://wholetale.org/){:target="_blank"}
- [Authorea](https://www.authorea.com/product){:target="_blank"}
- [RSpace](https://www.researchspace.com/){:target="_blank"}
- [Research Equals](https://libscie.org/researchequals-com/){:target="_blank"}



