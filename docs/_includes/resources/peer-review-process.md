## The Peer Review Process

### Incentivising Reviews {#incentivising-reviews}

The peer review system can be conceptualized as a “gift economy,” i.e., a system where previously published authors repay their debt for the labor that other scholars acting as editors and reviewers have invested in their manuscripts by acting as reviewers for others ([Kaltenbrunner et al., 2021](https://doi.org/10.1177/01622439211068798)). Despite this, it can be increasingly difficult to find reviewers for papers.

One way to incentivize reviews is to implement a way for authors to receive credit for the reviews they complete, and there are several ways to do this (see [EASE's guide to rewarding reviewers](https://ease.org.uk/rewarding-reviewers/)). One of the most widely known and used is [Publons](http://publons.com/) – a way to track an individual’s verified peer review history. [Reviewer Credits](https://www.reviewercredits.com/) is a similar initiative, except individuals can use their credits in a “Reward Center” to get actual rewards (ranging from discounted Article Processing Charges to online courses). Alternatively, reviewers can choose to [record their peer reviews on their ORCID profile](https://info.orcid.org/documentation/workflows/peer-review-workflow/). All of these require partnership with individual journals in order to be used by reviewers.

### Improving the quality of reviews {#improving-the-quality-of-reviews}

Although academics are expected to peer-review articles as part of their job, they often receive little (or no) formal training for this. Early career researchers are often keen to be involved in reviewing papers, but without having had many (or any) of their own papers reviewed, they don’t know what a review should look like. Here are some how-to guides from different fields that editors can share with their reviewers in order to help increase the quality of the reviews they receive.

- General: [Berk et al. (2015)](http://doi.org/10.2139/ssrn.2547191), [Faff (2018)](https://dx.doi.org/10.2139/ssrn.3269643), [Haggerty (2012)](https://www.chronicle.com/article/how-to-write-an-anonymous-peer-review/), [Hames (2016)](https://doi.org/10.6087/kcse.61), [Heddle et al., 2009](https://urldefense.com/v3/__https:/doi.org/10.1111/j.1537-2995.2009.02390.x__;!!KGKeukY!g5jqB6LdA31037oyGuu4_bl_NU4QP3TRwXZL-R7CEHoNquCpRfi-mAvqNa0kxZOk$), [Lucey (2013)](https://www.theguardian.com/higher-education-network/blog/2013/sep/27/peer-review-10-tips-research-paper), [McPeek et al. (2009)](https://doi.org/10.1086/598847), [Miller et al. (2013)](https://doi.org/10.1017/S104909651200128X), [Ngiam (2022)](https://williamngiam.github.io/blog/my_peer_review_process), [PLOS (2020)](https://plos.org/resource/how-to-read-a-manuscript-as-a-peer-reviewer/)

- Accounting: [Dalton et al. (2016)](https://doi.org/10.2308/iace-50979), [Kachelmeier (2004)](https://doi.org/10.2308/jata.2004.26.s-1.143), [Oler et al. (2016)](https://doi.org/10.2308/iace-50748)

- Computer Science: [Cormode, 2009](https://doi.org/10.1145/1519103.1519122)

- Ecology: [Scrimgeour et al. (2016)](https://doi.org/10.1086/688856)

- Economics: [Berk et al. (2017)](https://doi.org/10.1257/jep.31.1.231)

- Health: [Alexander (2005)](https://doi.org/10.1007/s10995-005-2423-y)

- Information Systems: [Hirschheim (2008)](https://doi.org/10.17705/1jais.00167)

- Management: [Carpenter (2009)](https://doi.org/10.5465/AMR.2009.36982609), [Lee (1995)](https://doi.org/10.1016/0272-6963(95)94762-W), [Lepak (2009)](https://doi.org/10.5465/AMR.2009.40631320), [Leung et al. (2014)](https://doi.org/10.1016/j.tmp.2014.01.003)

- Biomedical Science: [Christensen et al. (2010)](https://doi.org/10.1111/j.1442-2042.2010.02622.x), [Heddle & Ness (2009)](https://doi.org/10.1111/j.1537-2995.2009.02390.x), [Hoppin (2002)](https://doi.org/10.1164/rccm.200204-324OE), [Mayden (2012)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4093306/), [Rosenfeld (2010)](https://doi.org/10.1016/j.otohns.2010.02.010)

- Physiology: [Benos et al. (2003)](https://doi.org/10.1152/advan.00057.2002), [Guilford (2001)](https://doi.org/10.1152/advances.2001.25.3.167), [Seals et al. (2000)](https://doi.org/10.1152/advances.2000.23.1.s52)

- Political Science: [Esarey (2015)](https://thepoliticalmethodologist.files.wordpress.com/2016/02/tpm_v23_n1.pdf), [Hall et al.. (2019)](https://doi.org/10.1080/15236803.2019.1616657), [Krupnikov & Levine (2015)](https://thepoliticalmethodologist.files.wordpress.com/2016/02/tpm_v23_n1.pdf), [Miller at al. (2013)](https://doi.org/10.1017/S104909651200128X), [Nyhan (2015)](https://thepoliticalmethodologist.files.wordpress.com/2016/02/tpm_v23_n1.pdf)

- Psychology: [Epstein (1995)](https://doi.org/10.1037/0003-066X.50.10.883)

In addition to this, there are papers covering what _not_ to do as a reviewer, for example humiliate the authors ([Comer et al., 2014](https://doi.org/10.1080/17449642.2014.913341)) or be adversarial ([Cormode, 2009](https://doi.org/10.1145/1519103.1519122)).

Unique issues arise for reviewing interdisciplinary research – view and contribute to these conversations via this [working document](https://docs.google.com/document/d/1tTKbqEQ1gz5-8bZenyi4j2Tbnd0zvAuhmG805_S9NAs/edit).

For a [more complete bibliography organized alphabetically, see the list here](https://docs.google.com/document/d/1ZsxtLZBV2gcZ4UrsMg4UKy8zdCS5YHWIgTwMuOJo-y8/edit?usp=sharing).


### Peer Reviewing Tools & Guidelines {#peer-reviewing-tools-and-guidelines}

[EASE](https://ease.org.uk/) provides several guides for reviewers in their Peer Review Toolkit, including one on [how to write a review report](https://ease.org.uk/communities/peer-review-committee/peer-review-toolkit/how-to-write-a-review/) and a list of [peer review training options](https://ease.org.uk/communities/peer-review-committee/peer-review-toolkit/peer-review-training/).

One way editors can help reviewers is to make explicit the values and requirements of the journal so that reviewers can reflect these in their responses. For example, [*Language Development Research*](https://lps.library.cmu.edu/LDR/) makes it explicit in their [reviewer guidelines](https://lps.library.cmu.edu/LDR/site/reviewerguidelines/) that they wish to publish any research that meets their rigor criteria, without regard to the perceived novelty or importance of the findings.

The Society for Personality and Social Psychology created [guidelines for more inclusive reviewing practices](https://spsp.org/professional-development/publishing-resources/resources-for-inclusive-practices/guidelines-for-inclusive-reviewing-practices).

There are also tools being developed to guide peer reviewers in evaluating the manuscript, which journals can choose to implement. For example, [the INFORMS Journal on Data Science provides guidelines for reviewers](https://pubsonline.informs.org/page/ijds/reviewer-guidelines), and Elsevier makes available [a structured peer review question bank](https://www.elsevier.com/reviewers/how-to-review/structured-peer-review), from which journals may choose different questions to provide reviewers with as a way to guide their evaluation of each manuscript. In a similar spirit, [Schiavone et al. (2023)](https://psyarxiv.com/fc8v3/) have developed [an open-source tool for evaluating threats to the validity of empirical research (“Seaboat”)](https://www.seaboat.io/), which can generate reports to be shared alongside traditional peer reviews.

[Statcheck is also a popular tool for checking the consistency of reported statistics](https://michelenuijten.shinyapps.io/statcheck-web/), which can help journals, reviewers, and authors quickly and easily identify reporting errors that might otherwise go unnoticed.

### Encouraging authors to acknowledge limitations {#encouraging-authors-to-acknowledge-limitations}

It is important that authors are transparent about and own the limitations of their work. [Hoekstra and Vazire (2020)](https://doi.org/10.31234/osf.io/edh2s){:target="_blank"} provide a set of recommendations on how to increase intellectual humility in research articles that can be used as both author and reviewer guidelines. In addition, editors who want to incentivize intellectual humility in their journals can implement policies that make it clear to authors and reviewers that owning the limitations of one’s research will be considered a prerequisite for publication, rather than a possible reason to reject a manuscript. For an example of a policy like this, see the [reviewing policies](https://www.cambridge.org/core/journals/management-and-organization-review/collections/reviewing-policies-collection){:target="_blank"} for [*Management and Organization Review*](https://onlinelibrary.wiley.com/journal/17408784){:target="_blank"}. See also these two editorials from [*Nature Human Behavior*](https://www.nature.com/nathumbehav/){:target="_blank"} for short overviews of issues and solutions (“[Tell it like it is](https://www.nature.com/articles/s41562-020-0818-9){:target="_blank"}” and “[Not the first, not the best](https://www.nature.com/articles/s41562-021-01068-x){:target="_blank"}”).


### Peer Review Innovations {#peer-review-innovations}

Peer review is undergoing a sustained and multifaceted debate about different attempts to improve its perceived effectiveness, objectivity, transparency, and long-term sustainability. Many of these have already been discussed extensively above, e.g., deanonymizing reviews and/or reviewers, acknowledging and incentivizing reviewing, etc. There are many more – for an analytical overview of ongoing peer review innovations, see [Kaltenbrunner et al. (2022)](http://www.doi.org/10.31235/osf.io/8hdxu). They categorize peer review innovations as either being about the *object* of peer review (what is being peer reviewed), the *aim* of peer review (why is peer review being performed), the *role* of peer review *actors* (who performs peer review), the *nature* of peer review (how is peer review performed), or the *openness/transparency* of peer review (what is available to whom during and after peer review).

One such innovation is the peer review of preprints. [ASAPbio has some resources](https://asapbio.org/recognizing-preprint-peer-review) on how we can improve recognition of preprint peer review. Another initiative involves groups focused on facilitating the collective peer review of preprints in the format of journal clubs (i.e., where several people discuss one manuscript). [The Preprint Club](https://www.preprintclub.com) is an example of this approach in the medical field, while [PREreview Preprint Journal Clubs](https://prereview.org/preprint-journal-clubs) covers a wider variety of areas. In the case of PREreview, the output is a "full PREreview," or a citable online document published on [PREreview.org](https://prereview.org/), which can even be integrated into a journal's peer review process.

### Limitations of Peer Review {#limitations-of-peer-review}

It is important to acknowledge that peer review is an imperfect method for ensuring the credibility of published science. In fact, peer review became more popular after World War II partly for logistical reasons – as a means to manage increasing submissions ([Burnham, 1990](https://doi.org/10.1001/jama.1990.03440100023003)).

Although not a concrete piece of advice or "resource" as such, it can be important for journal editors to keep in mind debates surrounding the limitations of peer review. Some useful papers include: “Peer review: a flawed process at the heart of science and journals” ([Smith, 2006](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1420798/)), “The limitations to our understanding of peer review” ([Tennant & Ross-Hellauer, 2020](https://doi.org/10.1186/s41073-020-00092-1)), “Is Peer Review a Good Idea?” ([Heesen & Bright, 2021](https://doi.org/10.1093/bjps/axz029)), and “Academic urban legends” ([Rekdal, 2014](https://doi.org/10.1177/0306312714535679)).

One specific concern is that peer review may not be reliable or consistent (even with multiple reviewers per paper). Two papers showing this are: “The reliability of peer review for manuscript and grant submissions: A cross-disciplinary investigation” ([Cicchetti, 1991](https://doi.org/10.1017/S0140525X00065675)) and “Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment” ([Cortes & Lawrence, 2021](https://arxiv.org/abs/2109.09774)). It is debated whether this is a “feature or a bug,” and whether it is possible to achieve reliable and consistent peer review. One potential solution is collaborative review, where groups of reviewers rate manuscripts on multiple dimensions and then gather to discuss and adjust their respective ratings. This model has been used successfully in the [repliCATS project](https://replicats.research.unimelb.edu.au/) (part of the [SCORE program](https://www.darpa.mil/program/systematizing-confidence-in-open-research-and-evidence)).

Others challenge the "thinking against" approach to peer review and suggest a reorganization to a "thinking with" model, or "[care review](https://allegralaboratory.net/who-cares-peer-review-at-allegra/)" as Allegra Lab puts it. Read about this idea in the Emergent Conversation series on “[Peer Review as Intellectual Accompaniment](https://polarjournal.org/2022/06/13/peer-review-as-intellectual-accompaniment/),” including the introduction article “[Beyond the Courtroom Model of Intellectual Exchange in Peer Review](https://polarjournal.org/2022/06/13/introduction-thinking-with-when-peer-reviewing/).” A supportive and collaborative peer review model like this can foster a communal rather than competitive/combative academic culture, which is important for increasing participation from historically excluded groups ([Murphy et al., 2020](https://doi.org/10.1073/pnas.1921320117)).

Some believe there should be different norms for how papers can be written. For example, [Mastroianni & Ludwin-Peer (2022)](https://doi.org/10.31234/osf.io/2uxwk) published a preprint summarizing multiple studies in a style considered unconventional by traditional standards. In their words: _"The paper you just read could never be published in a scientific journal... You're supposed to be very serious... You're supposed to pack your paper with pointless citations because reviewers might like your paper more if they see their name in it. And if reviewers don't like your paper, they'll reject it, and nobody will ever see it..."_

There are several schools of thought regarding how to improve scientific peer review. [Waltman et al. (2022)](https://doi.org/10.31235/osf.io/v8ghj) proposed four: The Quality & Reproducibility school, the Democracy & Transparency school, the Equity & Inclusion school, and the Efficiency & Incentives school. Read this [blog post summarizing the preprint](https://blogs.lse.ac.uk/impactofsocialsciences/2022/03/24/there-are-four-schools-of-thought-on-reforming-peer-review-can-they-co-exist/).
