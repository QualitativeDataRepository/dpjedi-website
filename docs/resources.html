---
title: Data-PASS Journal Editors Discussion Interface
layout: main
---

<!-- Main -->
	<div id="main" class="wrapper style1">
		<div class="container">
			<section>
				<header class="major">
					<h1>Resources</h1>
					<a href="{{ site.baseurl }}/join.html" class="button alt">Join JEDI</a>&nbsp;&nbsp;
				<!--	<a href="https://groups.google.com/forum/#!forum/dpjedi" target="_blank" class="button alt" style="margin-bottom: 1.5em;">View JEDI Archives</a>-->
				</header>
				<p><em><strong>last updated 2021-05-03</strong></em></p>
				<p>JEDI members are jointly developing a collection of resources for journal editors in the social sciences. We are very grateful to JEDI members who have contributed resources for this page (see below for a list) and warmly welcome further suggestions. These can be made either by posting them to the mailing list, or by directly emailing Priya Silverstein at <a href="mailto:psilvers@syr.edu">psilvers@syr.edu</a>. Corresponding links will then be added to this page.</p>
				<p>Please check back here from time to time, as we hope this page will be updated regularly!</p>
				<p>If you are a journal editor and are not yet a member, please <a href="https://dpjedi.org/join.html" target="_blank">join JEDI</a>.</p>
				<h2 id="general">General</h2>
				<p>Publishers often offer guidelines and associated resources relating to editorial practices. While typically provided for editors of the journals that they publish, these resources may also be more generally helpful. For some examples see:</p>
				<ul class="simpleList">
				<li>
				<a href="https://journals.plos.org/plosone/s/resources-for-editors" target="_blank">PLOS ONE</a>
				</li>
				<li>
				<a href="https://us.sagepub.com/en-us/nam/resources-journal-authors-and-editors" target="_blank">SAGE</a>
				</li>
				<li>
				<a href="https://www.springer.com/gp/authors-editors/editors" target="_blank">Springer</a>
				</li>
				<li>
				<a href="https://editorresources.taylorandfrancis.com/" target="_blank">Taylor and Francis</a>
				</li>
				<li>
				<a href="https://authorservices.wiley.com/editors/index.html" target="_blank">Wiley</a>
				</li>
				</ul>
				<h2 id="incoming-editors">Incoming editors</h2>
				<p>If you are just starting out as a journal editor, you might find the <a href="https://publicationethics.org/" target="_blank">Committee on Publication Ethics’</a> short <a href="https://publicationethics.org/resources/guidelines-new/short-guide-ethical-editing-new-editors" target="_blank">guide to ethical editing for new editors</a> helpful, as well as this <a href="https://www.pauldudenhefer.net/glossary-of-publishing-and-editing-terms" target="_blank">glossary of publishing and editing terms</a>.</p>
				<p>The <a href="https://pkpschool.sfu.ca/about/" target="_blank">PKP school</a> also offers a free course in <a href="https://pkpschool.sfu.ca/courses/becoming-an-editor/" target="_blank">becoming an editor</a>, focusing on how to perform the major tasks required of an editor for a scholarly journal, how to analyze and solve common problems that may arise when editing a scholarly journal, how to assist other members of the journal team, and where to look for help with difficult issues.</p>
				<p>The <a href="https://www.councilscienceeditors.org/" target="_blank">Council of Science Editors</a> has <a href="https://www.councilscienceeditors.org/resource-library/editorial-policies/sample-correspondence-for-an-editorial-office/" target="_blank">sample correspondence for an editorial office</a> that you can customize to suit your journal.</p>
				<h2 id="ethics">Ethics</h2>
				<p>The <a href="https://publicationethics.org/" target="_blank">Committee on Publication Ethics</a> has many resources to help journal editors deal with ethical issues, including guidelines and case studies. For example, they have <a href="https://publicationethics.org/files/Systematic_manipulation_of_the_publication_process.pdf" target="_blank">guidelines on publication manipulation</a>.</p>
				<p>The <a href="https://www.councilscienceeditors.org/" target="_blank">Council of Science Editors</a> has a <a href="https://www.councilscienceeditors.org/resource-library/editorial-policies/white-paper-on-publication-ethics/" target="_blank">white paper on publication ethics</a> including a guide to <a href="https://www.councilscienceeditors.org/resource-library/editorial-policies/white-paper-on-publication-ethics/2-1-editor-roles-and-responsibilities/" target="_blank">editor roles and responsibilities</a>.</p>
				<h2 id="diversifying-social-science-research">Diversifying social science research</h2>
				<p>Systemic inequality exists within social science research. <a href="https://journals.sagepub.com/doi/10.1177/1745691620927709" target="_blank">Roberts et al. (2020)</a> examine racial inequality in psychological research to date and offer recommendations for editors and authors for working towards research that benefits from diversity in editing, writing, and participation. One recommendation is to require or offer the opportunity for authors to provide <a href="https://www.youtube.com/watch?v=GpcIVzGYhVs" target="_blank">positionality statements</a>, which are statements made following the process of <a href="https://www.utsc.utoronto.ca/~pchsiung/LAL/reflexivity" target="_blank">reflexivity</a>, whereby authors examine the “conceptual baggage” that they are bringing to the research. </p>
				<h2 id="open-science">Open science</h2>
				<p>A strong consensus is emerging in the social sciences and cognate disciplines that knowledge claims are more understandable and evaluable if scholars describe the research processes in which they engaged to generate them. Citing and showing the evidence on which claims rest (when this can be done within ethical and legal constraints), discussing the processes through which evidence was garnered, and explicating the analysis that produced the claims facilitate expression, interpretation, reproduction, and replication. The <a href="https://publicationethics.org/" target="_blank">Committee on Publication Ethics</a> has a list of <a href="https://publicationethics.org/resources/guidelines-new/short-guide-ethical-editing-new-editors" target="_blank">principles of transparency and best practice in scholarly publishing</a>.</p>
				<p><a href="https://science.sciencemag.org/content/348/6242/1422.full" target="_blank">Nosek et al. (2015)</a> presents an overview of the <a href="https://www.cos.io/initiatives/top-guidelines" target="_blank">Transparency and Openness Promotion (TOP) Guidelines</a> for journals, which have been used to generate the journal-level <a href="https://topfactor.org/" target="_blank">TOP Factor</a> and provide a clear view of areas in which editors can consider steps towards more open science at their journals. A similar initiative is the <a href="https://www.dartstatement.org/2014-journal-editors-statement-jets" target="_blank">DA-RT Journal Editors’ Transparency Statement (JETS)</a>.</p>
				<h3 id="resources-for-authors">Resources for authors</h3>
				<p>Aczel et al. (2020) present a consensus-based <a href="https://www.nature.com/articles/s41562-019-0772-6" target="_blank">checklist</a> to improve and document the transparency of research reports in social and behavioral research along with an <a href="http://www.shinyapps.org/apps/TransparencyChecklist/" target="_blank">online application</a> that allows users to complete the checklist and generate a report that they can submit with their manuscript or post to a public repository.</p>
				<h3 id="data-and-code">Data and code</h3>
				<p>A set of stable and easily adoptable core practices has begun to emerge with regard to data citation and management. For example, the social sciences are increasingly adopting the use of permanent identifiers, such as digital object identifiers (DOIs), for research products, articles and datasets. Similarly, there is now a strong consensus that sharing data via trusted digital repositories is preferable to doing so via personal websites.</p>
				<p>Journals are increasingly adopting data and code availability policies. The <a href="https://www.aeaweb.org/" target="_blank">American Economic Association</a> provides helpful <a href="https://aeadataeditor.github.io/aea-de-guidance/" target="_blank">guidance on implementation of their data and code availability policy</a> that could easily be applied to other journals and fields.</p>
				<p>For a discussion of the impact of journal data policy strictness on the code re-execution rate (i.e., how likely the code is to run without errors) and a set of recommendations for code dissemination aimed at journals, see <a href="https://arxiv.org/abs/2103.12793" target="_blank">Trisovic et al. (2020)</a>.</p>
				<h3 id="open-science-badges">Open science badges</h3>
				<p>One way of incentivizing open science is to offer open science badges to signal and reward when underlying data, materials, or preregistrations are available. Implementing badges is <a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456" target="_blank">associated with an increasing rate of data sharing</a>, as seeing colleagues practice open science signals that new community norms have arrived. See the <a href="https://www.cos.io/initiatives/badges" target="_blank">guidance on badges</a> by the <a href="http://www.cos.io" target="_blank">Center for Open Science</a> for more information on how to implement badges at your journal. However, it is important to note that receiving a badge for sharing data and code <a href="https://osf.io/srg57/" target="_blank">does not necessarily mean that analyses are reproducible</a> -- for this we turn to pre-publication verification of analyses.</p>
				<h3 id="pre-publication-verification-of-analyses">Pre-publication verification of analyses</h3>
				<p>Some journals have now adopted a policy whereby data and code are not only required for publication in the journal, but must be checked before publication to ensure that the analyses are reproducible -- that the results in the manuscript match the results that are produced when someone who is not one of the authors re-runs the code on the data. This is called pre-publication “verification of analyses”, “data and code replication”, or “reproduction of analyses”. For more information on how to implement a policy like this at your journal, see the <a href="https://social-science-data-editors.github.io/guidance/" target="_blank">Data and Code Guidance by Data Editors</a> developed by Lars Vilhuber and colleagues, which is used by the <a href="https://www.aeaweb.org/journals" target="_blank">American Economic Association journals</a>, <a href="https://onlinelibrary.wiley.com/journal/15405982" target="_blank">Canadian Journal of Economics</a>, the <a href="https://academic.oup.com/restud" target="_blank">Review of Economic Studies</a>, and the <a href="https://academic.oup.com/ej" target="_blank">Economic Journal</a> as a reference.</p>
				<h4 id="unshareable-data">Unshareable data</h4>
				<p>When data is unshareable (e.g. when data are sensitive), synthetic data can still be shared. This allows pre-publication verification of analyses to still take place. Dan Quintana has done a lot of work promoting the sharing of synthetic datasets and providing resources to help authors do so -- see his <a href="https://www.youtube.com/watch?v=0fAR_oro1NY" target="_blank">YouTube video</a>, <a href="https://www.dsquintana.blog/creating-and-synthetic-version-of-a-real-dataset/" target="_blank">blog post</a>, and <a href="http://doi.org/10.7554/eLife.53275" target="_blank">Quintana (2020)</a>.</p>
				<h3 id="verification-reports">Verification Reports</h3>
				<p>Verification Reports (VRs) are an article format focusing specifically on computational reproducibility and analytic robustness. VRs meet this objective by repeating the original analyses or reporting new analyses of original data. In doing so they provide the verifiers conducting the investigation with professional credit for evaluating one of the most fundamental forms of credibility: whether the claims in previous studies are justified by their own data. Chris Chambers has introduced this format at <a href="https://www.journals.elsevier.com/cortex" target="_blank">Cortex</a> (see his introductory <a href="https://doi.org/10.1016/j.cortex.2020.04.020" target="_blank">editorial</a>). For examples of the first two VRs published by Cortex , see <a href="https://doi.org/10.1016/j.cortex.2020.03.031" target="_blank">Chalkia et al. (2020)</a> and <a href="https://doi.org/10.1016/j.cortex.2021.01.017" target="_blank">Mirman et al. (2021)</a>. If you’re interested in including VRs as an article type at your journal, Cortex’s <a href="https://www.elsevier.com/__data/promis_misc/VR_GuideForAuthors.pdf" target="_blank">author guidelines</a> provide more information on this format.</p>
				<h3 id="registered-reports">Registered Reports</h3>
				<p><a href="https://www.cos.io/initiatives/registered-reports" target="_blank">Registered Reports</a> is a publishing format used by over 250 journals that emphasizes the importance of the research question and the quality of methodology by conducting peer review prior to data collection. High quality protocols are then provisionally accepted for publication if the authors follow through with the registered methodology. This format is designed to reward best practices in adhering to the hypothetico-deductive model of the scientific method. It eliminates a variety of questionable research practices, including low statistical power, selective reporting of results, and publication bias, while allowing complete flexibility to report serendipitous findings. Although Registered Reports are usually reserved for hypothesis-testing research, a version for exploratory research -- <a href="https://www.sciencedirect.com/science/article/pii/S0010945217302393?via%3Dihub" target="_blank">Exploratory Reports</a> -- is now also being offered.</p>
				<h4 id="resources-for-editors">Resources for editors</h4>
				<p>See the <a href="https://www.cos.io/initiatives/registered-reports" target="_blank">resources for editors</a> by the <a href="http://www.cos.io" target="_blank">Center for Open Science</a> for more information on implementing Registered Reports at your journal. More advice for reviewers and editors can be found in Box 2 and 3 of this <a href="https://osf.io/preprints/metaarxiv/43298/" target="_blank">preprint</a> by Chris Chambers and colleagues.</p>
				<h4 id="resources-for-authors-and-reviewers">Resources for authors and reviewers</h4>
				<p>The <a href="https://www.journals.elsevier.com/journal-of-development-economics" target="_blank">Journal of Development Economics</a> has developed a <a href="http://jde-preresultsreview.org/" target="_blank">website</a> dedicated to guidance for authors and reviewers on Registered Reports. Eike Rinke and colleagues at the <a href="https://www.cambridge.org/core/journals/journal-of-experimental-political-science" target="_blank">Journal of Experimental Political Science</a> have also created a great <a href="https://www.cambridge.org/core/journals/journal-of-experimental-political-science/information/faqs-for-registered-reports" target="_blank">FAQ page</a> for authors.</p>
				<h3 id="computational-research">Computational research</h3>
				<p><a href="https://hdsr.mitpress.mit.edu/pub/f0obb31j/release/1?readingCollection=c6cf45bb" target="_blank">Willis and Stodden (2020)</a> highlight nine decision points for journals looking to improve the quality and rigor of computational research and suggest that journals reporting computational research aim to include “assessable reproducible research artifacts” along with published articles.</p>
				<p>The <a href="https://ajps.org/ajps-verification-policy/" target="_blank">American Journal of Political Science Verification Policy</a> provides a role-model example of how computational research can be made to be more rigorous and error-free through additional steps in the editorial process – but it also shows how this requires resources on top of the procedures editors have become accustomed to over decades.</p>
				<h3 id="replication-studies">Replication studies</h3>
				<p>Many journals now have policies on publishing replication studies. Subscribing to the <a href="https://thehardestscience.com/2012/09/27/a-pottery-barn-rule-for-scientific-journals" target="_blank">“pottery barn rule”</a> means that journals agree to publish a direct replication of any study previously published in their journal. Other journals go beyond this, to agree to publish a replication of any study published in a major journal. In order to ensure that replications are assessed on the quality of their design rather than their results, a replication policy can include results blind review and/or be only for Registered Reports (see above). </p>
				<p><a href="https://royalsocietypublishing.org/journal/rsos" target="_blank">Royal Society Open Science</a> offers a great example of a <a href="https://royalsocietypublishing.org/rsos/replication-studies" target="_blank">replication policy</a> that adopts the (extended) pottery barn rule, and offers two tracks for review (results blind or Registered Report). See this <a href="https://royalsociety.org/blog/2018/10/reproducibility-meets-accountability/" target="_blank">blog post</a> introducing their policy. </p>
				<h2 id="improving-the-quality-of-reviews">Improving the quality of reviews</h2>
				<p>Although academics are expected to peer-review articles as part of their job, they often receive little (or no) formal training for this. Early career researchers are often keen to be involved in reviewing papers, but without having had many (or any) of their own papers reviewed, they don’t know what a review should look like. Here are some how-to guides from different fields that editors can share with their reviewers in order to help increase the quality of the reviews they recieve.</p>
				<ul class="simpleList">
				<li>
				General: <a href="http://doi.org/10.2139/ssrn.2547191" target="_blank">Berk et al. (2015)</a>, <a href="https://dx.doi.org/10.2139/ssrn.3269643" target="_blank">Faff (2018)</a>, <a href="https://doi.org/10.6087/kcse.61" target="_blank">Hames (2016)</a>, <a href="https://www.theguardian.com/higher-education-network/blog/2013/sep/27/peer-review-10-tips-research-paper" target="_blank">Lucey (2013)</a>, <a href="https://doi.org/10.1086/598847" target="_blank">McPeek et al. (2009)</a>
				</li>
				<li>
				Accounting: <a href="https://doi.org/10.2308/iace-50979" target="_blank">Dalton et al. (2016)</a>, <a href="https://doi.org/10.2308/jata.2004.26.s-1.143" target="_blank">Kachelmeier (2004)</a>, <a href="https://doi.org/10.2308/iace-50748" target="_blank">Oler et al. (2016)</a>
				</li>
				<li>
				Computer Science: <a href="https://doi.org/10.1145/1519103.1519122" target="_blank">Cormode, 2009</a>
				</li>
				<li>
				Ecology: <a href="https://doi.org/10.1086/688856" target="_blank">Scrimgeour et al. (2016)</a>
				</li>
				<li>
				Economics: <a href="https://doi.org/10.1257/jep.31.1.231" target="_blank">Berk et al. (2017)</a>
				</li>
				<li>
				Health: <a href="https://doi.org/10.1007/s10995-005-2423-y" target="_blank">Alexander (2005)</a>
				</li>
				<li>
				Information Systems: <a href="https://doi.org/10.17705/1jais.00167" target="_blank">Hirschheim (2008)</a>
				</li>
				<li>
				Management: <a href="https://doi.org/10.5465/AMR.2009.36982609" target="_blank">Carpenter (2009)</a>, <a href="https://doi.org/10.1016/0272-6963(95)94762-W" target="_blank">Lee (1995)</a>, <a href="https://doi.org/10.5465/AMR.2009.40631320" target="_blank">Lepak (2009)</a>, <a href="https://doi.org/10.1016/j.tmp.2014.01.003" target="_blank">Leung et al. (2014)</a>
				</li>
				<li>
				Biomedical Science: <a href="https://doi.org/10.1111/j.1442-2042.2010.02622.x" target="_blank">Christensen et al. (2010)</a>, <a href="https://doi.org/10.1111/j.1537-2995.2009.02390.x" target="_blank">Heddle &amp; Ness (2009)</a>, <a href="https://doi.org/10.1164/rccm.200204-324OE" target="_blank">Hoppin (2002)</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4093306/" target="_blank">Mayden (2012)</a>, <a href="https://doi.org/10.1016/j.otohns.2010.02.010" target="_blank">Rosenfeld (2010)</a>,
				</li>
				<li>
				Physiology: <a href="https://doi.org/10.1152/advan.00057.2002" target="_blank">Benos et al. (2003)</a>, <a href="https://doi.org/10.1152/advances.2001.25.3.167" target="_blank">Guilford (2001)</a>, <a href="https://doi.org/10.1152/advances.2000.23.1.s52" target="_blank">Seals et al. (2000)</a>
				</li>
				<li>
				Political Science: <a href="https://thepoliticalmethodologist.files.wordpress.com/2016/02/tpm_v23_n1.pdf" target="_blank">Esarey (2015)</a>, <a href="https://doi.org/10.1080/15236803.2019.1616657" target="_blank">Hall et al.. (2019)</a>, <a href="https://thepoliticalmethodologist.files.wordpress.com/2016/02/tpm_v23_n1.pdf" target="_blank">Krupnikov &amp; Levine (2015)</a>, <a href="https://doi.org/10.1017/S104909651200128X" target="_blank">Miller at al. (2013)</a>, <a href="https://thepoliticalmethodologist.files.wordpress.com/2016/02/tpm_v23_n1.pdf" target="_blank">Nyhan (2015)</a>
				</li>
				<li>
				Psychology: <a href="https://doi.org/10.1037/0003-066X.50.10.883" target="_blank">Epstein (1995)</a>
				</li>
				</ul>
				<p>In addition to this, there are papers covering what <em>not</em> to do as a reviewer, for example humiliate the authors (<a href="https://doi.org/10.1080/17449642.2014.913341" target="_blank">Comer et al., 2014)</a> or be adversarial (<a href="https://doi.org/10.1145/1519103.1519122" target="_blank">Cormode, 2009</a>).</p>
				<p>For a more complete bibliography organized alphabetically, see the list <a href="https://docs.google.com/document/d/1ZsxtLZBV2gcZ4UrsMg4UKy8zdCS5YHWIgTwMuOJo-y8/edit?usp=sharing" target="_blank">here</a>.</p>

				<h2 id="list-of-contributors-to-this-page-alphabetical-by-first-name">List of contributors to this page (alphabetical by first name)</h2>
				<ul class="simpleList">
					
				<li><a href="https://anatrisovic.com" target="_blank">Ana Trisovic</a></li>
				<li><a href="https://www.brown.edu/academics/population-studies/people/person/andrew-foster" target="_blank">Andrew Foster</a></li>
				<li><a href="https://asu.pure.elsevier.com/en/persons/ashley-randall" target="_blank">Ashley Randall</a></li>
				<li><a href="https://www.cardiff.ac.uk/people/view/133632-chambers-chris" target="_blank">Chris Chambers</a></li>
				<li><a href="https://www.maxwell.syr.edu/psc/Elman,_Colin" target="_blank">Colin Elman</a></li>
				<li><a href="http://faculty.usi.edu/cnsteltenp" target="_blank">Crystal Steltenpohl</a></li>
				<li><a href="https://gufaculty360.georgetown.edu/s/contact/00336000014TqNmAAK/diana-kapiszewski" target="_blank">Diana Kapiszewski</a></li>
				<li><a href="https://essl.leeds.ac.uk/politics/staff/1073/dr-eike-mark-rinke" target="_blank">Eike Rinke</a></li>
				<li><a href="https://nutrition.tufts.edu/profile/faculty/elena-n-naumova" target="_blank">Elena Naumova</a></li>
				<li><a href="https://sites.temple.edu/arceneaux" target="_blank">Kevin Arceneaux</a></li>
				<li><a href="https://www.vilhuber.com/lars" target="_blank">Lars Vilhuber</a></li>
				<li><a href="https://www.mdrc.org/about/michael-j-weiss" target="_blank">Michael Weiss</a></li>
				<li><a href="https://priyasilverstein.wixsite.com/website" target="_blank">Priya Silverstein</a></li></ul>

			
				</section>
		</div>
	</div>
