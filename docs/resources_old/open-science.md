---
title: Data-PASS Journal Editors Discussion Interface (JEDI) - Open science
layout: jedi
page_title: Resources
slug: open-science
sidebar: true
join: false
post: false
---
## Open science {#open-science}

A strong consensus is emerging in the social sciences and cognate disciplines that knowledge claims are more understandable and evaluable if scholars describe the research processes in which they engaged to generate them. Citing and showing the evidence on which claims rest (when this can be done within ethical and legal constraints), discussing the processes through which evidence was garnered, and explicating the analysis that produced the claims facilitate expression, interpretation, reproduction, and replication. The [Committee on Publication Ethics](https://publicationethics.org/){:target="_blank"} has a list of [principles of transparency and best practice in scholarly publishing](https://publicationethics.org/resources/guidelines-new/short-guide-ethical-editing-new-editors){:target="_blank"}.

[Nosek et al. (2015)](https://science.sciencemag.org/content/348/6242/1422.full){:target="_blank"} presents an overview of the [Transparency and Openness Promotion (TOP) Guidelines](https://www.cos.io/initiatives/top-guidelines){:target="_blank"} for journals, which have been used to generate the journal-level [TOP Factor](https://topfactor.org/){:target="_blank"} and provide a clear view of areas in which editors can consider steps towards more open science at their journals. See this [summary table](https://osf.io/2cz65/){:target="_blank"} and example policy text for achieving a TOP level of [1](https://osf.io/t8f2k/){:target="_blank"}, [2](https://osf.io/7ebwk/){:target="_blank"}, or [3](https://osf.io/phbq9/){:target="_blank"} for different standards. For an example of how to explicitly signal adherence to TOP guidelines at your journal, see this example [policy](https://www.elsevier.com/__data/promis_misc/Cortex-TOP-author-guidelines.pdf){:target="_blank"} and [author checklist](https://osf.io/5stjg){:target="_blank"} from [*Cortex*](https://www.journals.elsevier.com/cortex){:target="_blank"}. You can even [contact the policy team](https://www.cos.io/contact-us){:target="_blank"} at the [Center for Open Science](https://www.cos.io/){:target="_blank"} to talk through your journal’s specific needs with regards to editing your policies and practices in line with TOP Guidelines. A similar initiative is the [DA-RT Journal Editors’ Transparency Statement](https://www.dartstatement.org/2014-journal-editors-statement-jets){:target="_blank"} (JETS), a product of the [American Political Science Association](https://www.apsanet.org/ABOUT/About-APSA){:target="_blank"}’s 2010 [Data Access and Research Transparency initiative](https://www.dartstatement.org/about){:target="_blank"}.

### Resources for authors {#resources-for-authors}

Aczel et al. (2020) present a consensus-based [checklist](https://www.nature.com/articles/s41562-019-0772-6){:target="_blank"} to improve and document the transparency of research reports in social and behavioral research along with an [online application](http://www.shinyapps.org/apps/TransparencyChecklist/){:target="_blank"} that allows users to complete the checklist and generate a report that they can submit with their manuscript or post to a public repository.

### Data and code {#data-and-code}

A set of stable core practices has begun to emerge with regard to data management and sharing. While all are readily available, some require more effort on the part of journals. For example, requiring authors to share data via a trusted digital repository (and not e.g. via a personal website) is easier than checking that the author's code runs. Iain Hrynaszkiewicz ([*Public Library of Science*](https://plos.org/){:target="_blank"}) has written a chapter outlining “[Publishers’ Responsibilities in Promoting Data Quality and Reproducibility](https://doi.org/10.1007/164_2019_290){:target="_blank"}” that describes practical approaches being taken by publishers to promote rigor and transparency in data practices.

Journals are increasingly adopting data and code availability policies. The [Research Data Alliance](https://rd-alliance.org/){:target="_blank"} has developed a [Research Data Policy Framework](https://datascience.codata.org/article/10.5334/dsj-2020-005/){:target="_blank"} for all journals and publishers including template policy texts which can be implemented by journals in their Information for Authors and publishing workflows.

There are many repositories to choose from, and different repositories will have benefits for different fields and needs. For lists of recommended repositories, see for example: [section 2.2.1 in *F1000 Research*’s data guidelines](https://f1000research.com/for-authors/data-guidelines){:target="_blank"}; [Springer Nature’s list of social science repository examples](https://www.springernature.com/gp/authors/research-data-policy/repositories-socsci/12327116){:target="_blank"}; and [*PLOS One*’s recommended repositories](https://journals.plos.org/plosone/s/recommended-repositories){:target="_blank"}. Similarly, the [*Society for Social and Personality Psychology*](https://www.spsp.org/publications/guidelines-resources#4){:target="_blank"} has created a matrix of different trusted repositories and what they offer. Similarly, [re3data](https://www.re3data.org/browse/by-subject/){:target="_blank"} provides a database of different repositories and their attributes categorized by subject, content type, and country.

[Goodman et al. (2014)](https://doi.org/10.1371/journal.pcbi.1003542){:target="_blank"} states that “A proper, trustworthy archive will: (1) assign an identifier such as a “handle” (hdl) or “digital object identifier” (doi) (see the [DOI handbook](https://www.doi.org/doi_handbook/6_Policies.html#6.5){:target="_blank"}); (2) require that you provide adequate documentation and metadata; and (3) manage the “care and feeding” of your data by employing good curation practices”. The [Research Data Alliance](https://www.rd-alliance.org/){:target="_blank"} have created [10 Things for Curating Reproducible and FAIR Research](https://www.rd-alliance.org/group/cure-fair-wg/outcomes/10-things-curating-reproducible-and-fair-research){:target="_blank"} and [The CoreTrustSeal Trustworthy Data Repositories Requirements](https://doi.org/10.5281/zenodo.3638211){:target="_blank"}. See also the [FAIR Guiding Principles](https://www.go-fair.org/fair-principles/){:target="_blank"} for data management and stewardship.

In some fields (e.g. economics), it is common practice to have on the editorial team a specific “data editor”. These data editors are responsible for creating, adapting, and implementing data and code sharing policies at their respective journals. Data editors often share important information that can be useful for other editors looking to adopt or adapt their existing data and code sharing policies. For example, the data editor websites for the [*American Economic Association*](https://aeadataeditor.github.io/){:target="_blank"}, [*The Review of Economic Studies*](https://restud.github.io/data-editor/){:target="_blank"}, and [*The Economic Journal*](https://ejdataeditor.github.io/){:target="_blank"} all offer a wealth of information and advice. See, for example, the [*American Economic Association*](https://aeadataeditor.github.io/){:target="_blank"} policy on [revisions of data and code deposits](https://www.aeaweb.org/journals/data/policy-revisions){:target="_blank"} in the AEA data and code repository.

### Data ethics {#data-ethics}

It is important to note that there will be cases where data cannot be shared, and that it is important to be “as open as possible, as closed as necessary”. See [Meyer (2018)](https://doi.org/10.1177/2515245917747656){:target="_blank"} for an excellent guide on “ethical data sharing”. GREI (the NIH Generalist Repository Ecosystem Initiative) also has [a webinar series](https://datascience.nih.gov/grei-collaborative-webinar-series){:target="_blank"} on this topic.

[FORCE11](https://www.force11.org/){:target="_blank"} and [COPE](https://publicationethics.org/){:target="_blank"} have developed some recommendations for the handling of ethical concerns relating to the publication of research data. See their [blog post](https://www.force11.org/article/recommendations-handling-ethical-concerns-relating-publication-research-data){:target="_blank"} and the [recommendations](https://zenodo.org/record/5391293#.YYATD9bP0-Q){:target="_blank"} themselves here.

There may be particular types of data that are more difficult to share – e.g., data on sensitive topics. For a case study that includes challenges, tools, and future directions of sharing data in these cases, please see [Joel et al. (2018)](https://doi.org/10.1177%2F2515245917744281){:target="_blank"}. For a case study that discusses the redaction of sensitive data, see [Casadevall et al. (2013)](https://doi.org/10.1128/mBio.00991-13){:target="_blank"}. Many repositories offer restricted access to data, e.g. the repositories organized in [Data-PASS](http://www.data-pass.org){:target="_blank"} (e.g. [ICPSR](https://www.icpsr.umich.edu/web/pages/){:target="_blank"}, [QDR](https://qdr.syr.edu/){:target="_blank"}, [Odum](https://odum.unc.edu/archive/){:target="_blank"}, [Databrary](https://nyu.databrary.org/){:target="_blank"}), and almost all repositories organized in [CESSDA](https://www.cessda.eu/){:target="_blank"}.

### Open data {#open-data}

Mandating or encouraging authors to share data alongside their manuscripts means that reviewers (and later, readers) can:

- See the structure of the data more clearly
- Run additional analyses
- Use the data to answer new questions

In addition, there may be a citation advantage for authors sharing data ([Colavizza et al., 2020](https://doi.org/10.1371/journal.pone.0230416){:target="_blank"}).

### Open data and code {#open-data-and-code}

Mandating or encouraging authors to share both data and code alongside their manuscripts has all the above benefits of sharing either of these separately, but also means that reviewers (and later, readers) can check the computational reproducibility of the results (i.e., does running the code on the data produce the same results that are reported in the paper).

The [American Economic Association](https://www.aeaweb.org/){:target="_blank"} provides helpful [guidance on implementation of their data and code availability policy](https://aeadataeditor.github.io/aea-de-guidance/){:target="_blank"} that could easily be applied to other journals and fields. [The Berkeley Institute for Transparency in the Social Sciences](https://www.bitss.org/){:target="_blank"} offers a handy [guide on conducting reproducibility checks](https://bitss.github.io/ACRE/){:target="_blank"}.

For a discussion of the impact of journal data policy strictness on the code re-execution rate (i.e., how likely the code is to run without errors) and a set of recommendations for code dissemination aimed at journals, see [Trisovic et al. (2020)](https://arxiv.org/abs/2103.12793){:target="_blank"}.

### Pre-publication verification of analyses {#pre-publication-verification-of-analyses}

Some journals have now adopted a policy whereby data and code are not only required for publication in the journal, but must be checked before publication to ensure that the analyses are reproducible – that the results in the manuscript match the results that are produced when someone who is not one of the authors re-runs the code on the data. This is called pre-publication “verification of analyses”, “data and code replication”, or “reproduction of analyses”. See [Willis & Stodden (2020)](https://doi.org/10.1162/99608f92.25982dcf){:target="_blank"} for a useful overview of how to leverage policies, workflows, and infrastructure to ensure computational reproducibility in publication.

For more information on how to implement a policy like this at your journal, see the [Data and Code Guidance by Data Editors](https://social-science-data-editors.github.io/guidance/){:target="_blank"} developed by Lars Vilhuber and colleagues, which is used by the [American Economic Association journals](https://www.aeaweb.org/journals){:target="_blank"}, [Canadian Journal of Economics](https://onlinelibrary.wiley.com/journal/15405982){:target="_blank"}, the [Review of Economic Studies](https://academic.oup.com/restud){:target="_blank"}, and the [Economic Journal](https://academic.oup.com/ej){:target="_blank"} as a reference. This [practical guide on training students to assess the reproducibility of articles in the social sciences](https://labordynamicsinstitute.github.io/ldilab-manual/index.html){:target="_blank"} may also be of interest.

Several journals in political science also require pre-publication verification. See for example [State Politics & Policy Quarterly](https://www.cambridge.org/core/journals/state-politics-and-policy-quarterly/information/instructions-contributors){:target="_blank"}, [Political Analysis](https://www.cambridge.org/core/journals/political-analysis/information/instructions-contributors){:target="_blank"}, and the [American Journal of Political Science](https://ajps.org/){:target="_blank"}.

### Resources for open research planning & documentation {#resources-open-research-plan-doc}

Many platforms can aid researchers in making their entire workflows open and transparent. Here are a few examples:

- [Code Ocean](http://codeocean.com/){:target="_blank"}
- [The Whole Tale](http://wholetale.org/){:target="_blank"}
- [Authorea](https://www.authorea.com/product){:target="_blank"}
- [RSpace](https://www.researchspace.com/){:target="_blank"}
- [Research Equals](https://libscie.org/researchequals-com/){:target="_blank"}


### Data citation {#data-citation}

The social sciences are increasingly adopting the use of permanent identifiers, such as digital object identifiers (DOIs), for datasets, making it easier to find and cite data sources. There may be additional stylistic guidelines that are discipline-specific, for example this [guide to dataset references](https://apastyle.apa.org/style-grammar-guidelines/references/examples/data-set-references){:target="_blank"} from the [American Psychological Association](https://apastyle.apa.org/){:target="_blank"}.

Many guides for data citation exist that can be shared with authors and/or adopted as a policy at your journal:

- [TOP Guidelines](https://www.cos.io/initiatives/top-guidelines){:target="_blank"} (section on Citation Standards)
- [Joint Declaration of Data Citation Principles](https://doi.org/10.25490/a97f-egyk){:target="_blank"}
- [Guidance on Data Citations](https://social-science-data-editors.github.io/guidance/addtl-data-citation-guidance.html#many-related-datasets){:target="_blank"} (including a [widget for computing citations](https://social-science-data-editors.github.io/guidance/addtl-data-citation-guidance.html#try-it-out){:target="_blank"})
- [DataCite](https://datacite.org/){:target="_blank"} (including [DataCite Metadata Schema](https://schema.datacite.org/){:target="_blank"})
- [How to Cite Data and Statistics](https://guides.nyu.edu/datasources/data-citation){:target="_blank"}
- [Citing data sources – Why is it good and how to do it?](https://library.cumc.columbia.edu/insight/citing-data-sources){:target="_blank"}

[DataSeer](https://dataseer.ai/){:target="_blank"} is intended to help journals flag references to data in manuscripts, which could be used to ensure all data referenced are being properly cited.


### Heterogeneous data and analytic materials {#heterogeneous-data-and-analytic-materials}

While this section is framed in terms of numeric data and computer code, it is worth noting that cognate considerations arise in all circumstances where authors use a combination of data and analytic reasoning to make their findings. For example, authors can also make qualitative data and materials available for case studies that used process tracing analyses and relied on interviews and archival data.

### Open science badges {#open-science-badges}

One way of incentivizing open science is to offer open science badges to signal and reward when underlying data, materials, or preregistrations are available. Implementing badges is [associated with an increasing rate of data sharing](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456){:target="_blank"}, as seeing colleagues practice open science signals that new community norms have arrived. See the [guidance on badges](https://www.cos.io/initiatives/badges){:target="_blank"} by the [Center for Open Science](http://www.cos.io){:target="_blank"} for more information on how to implement badges at your journal. However, it is important to note that [receiving a badge for sharing data and code does not necessarily mean that analyses are reproducible](https://osf.io/srg57/){:target="_blank"}.

### Verification Reports {#verification-reports}

Verification Reports (VRs) are an article format focusing specifically on computational reproducibility and analytic robustness. VRs meet this objective by repeating the original analyses or reporting new analyses of original data. In doing so they provide the verifiers conducting the investigation with professional credit for evaluating one of the most fundamental forms of credibility: whether the claims in previous studies are justified by their own data. Chris Chambers has introduced this format at [*Cortex*](https://www.journals.elsevier.com/cortex){:target="_blank"} (see his introductory [editorial](https://doi.org/10.1016/j.cortex.2020.04.020){:target="_blank"} and [materials for journals](https://osf.io/9g5d8/){:target="_blank"}). For examples of the first two VRs published by *Cortex*, see [Chalkia et al. (2020)](https://doi.org/10.1016/j.cortex.2020.03.031){:target="_blank"} and [Mirman et al. (2021)](https://doi.org/10.1016/j.cortex.2021.01.017){:target="_blank"}. If you’re interested in including VRs as an article type at your journal, Cortex’s [author guidelines](https://www.elsevier.com/__data/promis_misc/VR_GuideForAuthors.pdf){:target="_blank"} provide more information on this format.

### Preregistration {#preregistration}

Preregistering research means specifying a research plan in advance of a study and submitting it to a registry. Preregistration separates hypothesis-generating (exploratory) from hypothesis-testing (confirmatory) research. See the [Center for Open Science](https://www.cos.io/){:target="_blank"}’s [guide to preregistration](https://www.cos.io/initiatives/prereg){:target="_blank"} for more information. Journals can choose to encourage, reward, or require preregistration for confirmatory research.

There are several independent, institutional registries for preregistering research; for example, [ClinicalTrials](http://clinicaltrials.gov/){:target="_blank"}, [American Economic Association Randomised Control Trial Registry](https://www.socialscienceregistry.org/){:target="_blank"}, [Open Science Framework](http://openscienceframework.org/){:target="_blank"}, [EGAP Registry](https://egap.org/registry-0/){:target="_blank"}, and [The Registry for International Development Impact Evaluations](https://ridie.3ieimpact.org/){:target="_blank"}.

There are many templates available for preregistering different types of research. For example, the [Center for Open Science](https://www.cos.io/){:target="_blank"} has [several options available here](https://osf.io/registries/osf/new?view_only=){:target="_blank"}, including more general templates and templates specific to social psychology, qualitative research, and secondary data. See [van den Akker et al. (2021)](https://osf.io/preprints/metaarxiv/3nbea/){:target="_blank"} for a systematic review preregistration template.

If you choose to encourage, reward, or require preregistration for confirmatory research at your journal, you can think about whether to suggest or require that authors use a specific registry or template for studies submitted to your journal.

### Registered Reports {#registered-reports}

[Registered Reports](https://www.cos.io/initiatives/registered-reports){:target="_blank"} (RR) is a publishing format used by over 250 journals that emphasizes the importance of the research question and the quality of methodology by conducting peer review prior to data collection. High-quality protocols are then provisionally accepted for publication if the authors follow through with the registered methodology. This format is designed to reward best practices in adhering to the hypothetico-deductive model of the scientific method. It eliminates a variety of questionable research practices, including low statistical power, selective reporting of results, and publication bias, while allowing complete flexibility to report serendipitous findings. Although RRs are usually reserved for hypothesis-testing research, a version for exploratory research – [Exploratory Reports](https://www.sciencedirect.com/science/article/pii/S0010945217302393?via%3Dihub){:target="_blank"} – is now also being offered.

RRs are a new and evolving format, and so many people are working to improve RRs for authors, reviewers, and editors. To read a summary of these conversations, or add to them, please see this [working document](https://bit.ly/RRsSIPS){:target="_blank"}.

#### Resources for editors {#resources-for-editors}

See the [resources for editors](https://www.cos.io/initiatives/registered-reports){:target="_blank"} by the [Center for Open Science](http://www.cos.io/){:target="_blank"} for more information on implementing Registered Reports at your journal. More advice for reviewers and editors can be found in Box 2 and 3 of this [preprint](https://osf.io/preprints/metaarxiv/43298/){:target="_blank"} by Chris Chambers and colleagues. For an example of an RR policy, see the [RR policy](http://cdn.elsevier.com/promis_misc/PROMIS%20pub_idt_CORTEX%20Guidelines_RR_29_04_2013.pdf){:target="_blank"} for [*Cortex*](https://www.journals.elsevier.com/cortex){:target="_blank"}.

#### Resources for authors and reviewers {#resources-for-authors-and-reviewers}

The [*Journal of Development Economics*](https://www.journals.elsevier.com/journal-of-development-economics){:target="_blank"} has developed a [website dedicated to guidance for authors and reviewers on Registered Reports](http://jde-preresultsreview.org/){:target="_blank"}. Eike Rinke and colleagues at the [*Journal of Experimental Political Science*](https://www.cambridge.org/core/journals/journal-of-experimental-political-science){:target="_blank"} have also created a great [FAQ page](https://www.cambridge.org/core/journals/journal-of-experimental-political-science/information/faqs-for-registered-reports){:target="_blank"} for authors.

#### PCI Registered Reports {#pci-registered-reports}

An exciting new initiative – [Peer Community In Registered Reports](https://rr.peercommunityin.org/){:target="_blank"} (PCI-RR) offers free and transparent pre- and post-study recommendations, managing the peer review of Registered Report preprints. The peer review is independent of journals but is endorsed by a growing list of journals that accept PCI-RR recommendations. Read [about PCI-RR](https://rr.peercommunityin.org/about/about){:target="_blank"} and see the [PCI-RR Journal Adopter FAQ](https://rr.peercommunityin.org/about/journal_adopter_faq){:target="_blank"} for more information.

### Open Peer Review {#open-peer-review}

“Open review” means different things depending on what is open and to whom. Open review is a good example of a situation where the most open version may not be the fairest or best option, as there are many factors to take into account. It is also important to distinguish between open identities and open reviews (both of which, confusingly, can be called open reviewing). For an overview of the various definitions of Open Peer Review, see [Ross-Hellauer (2017)](https://f1000research.com/articles/6-588/v2){:target="_blank"}. For a review of the benefits and limitations of open peer review (some outlined below), see [Besançon et al. (2020)](https://doi.org/10.1186/s41073-020-00094-z){:target="_blank"}. For guidelines on implementing open peer review, see [Ross-Hellauer and Görögh (2019)](https://doi.org/10.1186/s41073-019-0063-9){:target="_blank"}.

#### Open identities {#open-identities}

Journals will have a policy on whether editors, reviewers, and authors know each other’s identities. [Transpose](https://transpose-publishing.github.io/#/){:target="_blank"} (TRANsparency in Scholarly Publishing for Open Scholarship Evolution) offers a database that includes information about which type of peer review different journals are currently using.

In **open reviewing** everyone knows the identity of everyone. This is argued to increase the accountability of the reviewer, giving less scope for biased or unjustified judgments. [Godlee et al. (2002)](https://doi.org/10.1001/jama.287.21.2762){:target="_blank"} offers a good introduction into the benefits of making reviewers open.

In a **single-masked** system, only reviewers are made anonymous. This is to be as open as possible, while ensuring that reviewers are not treated unfairly (or fear being treated unfairly) for giving unfavorable reviews. Indeed, it has been found that reviewers are less likely to express criticism ([Mulligan et al., 2013](https://doi.org/10.1002/asi.22798){:target="_blank"}; [Ross-Hellauer et al., 2017](https://doi.org/10.1371/journal.pone.0189311){:target="_blank"}) and are less likely to recommend rejecting articles ([Bravo et al., 2019](https://doi.org/10.1038/s41467-018-08250-2){:target="_blank"}; [Bruce et al., 2016](https://doi.org/10.1186/s12916-016-0631-5){:target="_blank"}; [Sambeek & Lakens, 2021](https://doi.org/10.15626/MP.2019.2289){:target="_blank"}; [Walsh et al., 2000](https://doi.org/10.1192/bjp.176.1.47){:target="_blank"}) if their identity is known to authors.

In a **double-masked** system, the editor knows the identities of reviewers and authors, reviewers and authors both know the identity of the editor, but authors and reviewers do not know the identity of each other. This is an attempt to eliminate personal biases of reviewers (e.g. those based on gender, seniority, reputation, and affiliation), e.g. see [Tomkins et al. (2017)](https://doi.org/10.1073/pnas.1707323114){:target="_blank"} for evidence of single-masked review being biased towards papers with famous authors and authors from high-prestige institutions. See [Nature’s editorial adopting an optionally double-masked system](https://www.nature.com/articles/518274b){:target="_blank"}. [Kern-Goldberger et.al. (2022)](https://pubmed.ncbi.nlm.nih.gov/35120887/){:target="_blank"} have conducted a systematic review on the impact of double-masked peer review on gender bias in scientific publishing, and studies seem to demonstrate mixed results. When given the choice, corresponding authors from less prestigious institutions are more likely to choose double-masked review ([McGillivray & De Ranieri, 2018](https://doi.org/10.1186/s41073-018-0049-z){:target="_blank"}).

In a **triple-masked** system, the identity of the editor, authors, and reviewers are all masked from each other. This acknowledges the possible personal biases of editors as well as reviewers. For some examples of triple-masked review in practice, see the guidelines for authors from [*Comparative Political Studies*](https://journals.sagepub.com/author-instructions/CPS){:target="_blank"} and [*Perspectives on Politics*](https://www.apsanet.org/perspectivessubmissions){:target="_blank"}.

There is no one-size-fits-all solution, but it is important to think carefully about which policy makes sense for your journal. If you choose a single-masked or double-masked system, you also need to think about what to do if authors choose to sign their reviews. Some journals remove these signatures, and some choose to allow authors to do this if they wish. If your field has a high percentage of desk rejections (e.g., Political Science: [Garand & Harman, 2021](https://doi.org/10.1017/S1049096521000573){:target="_blank"}) then you may wish to consider double-masked review as editor bias would have a bigger impact in your field.

#### Open reviews {#open-reviews}

It is also possible to make the reviews themselves openly available alongside published manuscripts (with or without the reviewers being identified). This can help to give context to the published article. See this [blog post discussing the launch of open reviews](https://www.issi-society.org/blog/posts/2020/september/quantitative-science-studies-launches-transparent-peer-review-pilot/){:target="_blank"} at [*Quantitative Science Studies*](https://direct.mit.edu/qss){:target="_blank"}. [EASE](https://ease.org.uk/){:target="_blank"} provides a history of why some journals are making reviews open as well as a [guide on how reviewers can publish their own review reports](https://ease.org.uk/communities/peer-review-committee/peer-review-toolkit/how-to-publish-review-reports/){:target="_blank"}. The [Publish Your Reviews](https://asapbio.org/publishyourreviews){:target="_blank"} initiative encourages peer reviewers to publish their reviews alongside the preprint of an article.

### Open access {#open-access}

Open access (making research outputs freely available to all) is important for disseminating and sharing scientific results with scientists and members of the public around the world. See [The Turing Way](https://the-turing-way.netlify.app/welcome.html){:target="_blank"}’s [guide to open access](https://the-turing-way.netlify.app/reproducible-research/open/open-access.html){:target="_blank"} for an explanation of different versions of Open Access. Essentially, journals can publish open access articles in a hybrid model (where not all articles are open access, but some are), or be fully open access (where all articles are open access). Journals and publishers decide how to fund this – usually through charging authors for making their articles open access.

Although many journals are now open access from the outset, many journals that previously did not offer open access are making/have made the transition to some form of open access. For example, the editorial team from the journal *Lingua* [broke off from Elsevier](https://www.kaivonfintel.org/lingua-glossa/){:target="_blank"} and launched a fully open access journal: [*Glossa*](https://www.glossa-journal.org/){:target="_blank"}. Those interested in taking similar steps may wish to read about their journey; here are [some slides on the transition](https://scholarsarchive.library.albany.edu/cgi/viewcontent.cgi?article=1007&context=open_access_week){:target="_blank"}, a [blog post](https://www.rooryck.org/lingua-to-glossa){:target="_blank"} by one of the previous Executive Editors, and an [interview with one of the previous associate editors](https://www.universityaffairs.ca/news/news-article/a-behind-the-scenes-look-at-the-mass-resignations-at-lingua/){:target="_blank"}.

#### Becoming an independent open access journal {#becoming-an-independent-open-access-journal}

The PCI (Peer Community In) [held a webinar in March of 2023](https://www.youtube.com/watch?v=TzLgGSNq0Wk){:target="_blank"} outlining the current publishing landscape. It includes information on the ethics of different kinds of publishing models, including for-profit vs. non-profit journals, and different levels and models of open access publishing, which might be useful in thinking through some of the advantages and challenges of becoming an open access independent journal. SPARC also has a [website dedicated to helping journals declare independence](https://declaring-independence.org/){:target="_blank"}.

There are some outstanding examples of existing journals shifting from a commercial publisher or university press to an independent fully open access journal, or new independent fully open access journals being founded. See some of the below examples for inspiration! It is important to consider how/whether your journal will be able to be indexed, for example in Web of Science ([see here for their evaluation process and selection criteria](https://clarivate.com/products/scientific-and-academic-research/research-discovery-and-workflow-solutions/web-of-science/core-collection/editorial-selection-process/editorial-selection-process/){:target="_blank"}). If this process involves transitioning to another publisher, [NISO has some guidelines](https://www.niso.org/standards-committees/transfer){:target="_blank"} to ensure the transfer process occurs with minimum disruptions.

The editorial team from the journal *Lingua* [broke off from Elsevier](https://www.kaivonfintel.org/lingua-glossa/){:target="_blank"} and launched [*Glossa*](https://www.glossa-journal.org/){:target="_blank"}. Here are some [slides on the transition](https://scholarsarchive.library.albany.edu/cgi/viewcontent.cgi?article=1007&context=open_access_week){:target="_blank"}, a [blog post](https://www.rooryck.org/lingua-to-glossa){:target="_blank"} by one of the previous Executive Editors, and an [interview on the transition](https://www.universityaffairs.ca/news/news-article/a-behind-the-scenes-look-at-the-mass-resignations-at-lingua/){:target="_blank"}.

The editorial team from the *Journal of Informetrics* also broke off from Elsevier and launched [*Quantitative Science Studies*](https://direct.mit.edu/qss){:target="_blank"}. See this [article about the transition](https://www.insidehighered.com/news/2019/01/14/elsevier-journal-editors-resign-start-rival-open-access-journal){:target="_blank"}.

The [*Journal of Privacy and Confidentiality*](https://journalprivacyconfidentiality.org/index.php/jpc){:target="_blank"} made the shift to being an independent journal – see these 2018 editorials on their relaunch ([Vilhuber, 2018](https://doi.org/10.29012/jpc.706){:target="_blank"}) and future ([Dwork, 2018](https://doi.org/10.29012/jpc.708){:target="_blank"}). In the next phase, they are currently working on creating a non-profit organization that will house the journal.

### Computational research {#computational-research}

[Willis and Stodden (2020)](https://hdsr.mitpress.mit.edu/pub/f0obb31j/release/1?readingCollection=c6cf45bb){:target="_blank"} highlight nine decision points for journals looking to improve the quality and rigor of computational research and suggest that journals reporting computational research aim to include “assessable reproducible research artifacts” along with published articles.

The [*American Journal of Political Science* Verification Policy](https://ajps.org/ajps-verification-policy/){:target="_blank"} provides a role-model example of how computational research can be made more rigorous and error-free through additional steps in the editorial process – but it also shows how this requires resources on top of the procedures editors have become accustomed to over decades.

### Encouraging authors to acknowledge limitations {#encouraging-authors-to-acknowledge-limitations}

It is important that authors are transparent about and own the limitations of their work. [Hoekstra and Vazire (2020)](https://doi.org/10.31234/osf.io/edh2s){:target="_blank"} provide a set of recommendations on how to increase intellectual humility in research articles that can be used as both author and reviewer guidelines. In addition, editors who want to incentivize intellectual humility in their journals can implement policies that make it clear to authors and reviewers that owning the limitations of one’s research will be considered a prerequisite for publication, rather than a possible reason to reject a manuscript. For an example of a policy like this, see the [reviewing policies](https://www.cambridge.org/core/journals/management-and-organization-review/collections/reviewing-policies-collection){:target="_blank"} for [*Management and Organization Review*](https://onlinelibrary.wiley.com/journal/17408784){:target="_blank"}. See also these two editorials from [*Nature Human Behavior*](https://www.nature.com/nathumbehav/){:target="_blank"} for short overviews of issues and solutions (“[Tell it like it is](https://www.nature.com/articles/s41562-020-0818-9){:target="_blank"}” and “[Not the first, not the best](https://www.nature.com/articles/s41562-021-01068-x){:target="_blank"}”).

### Replication studies {#replication-studies}

Many journals now have policies on publishing replication studies. Subscribing to the [“pottery barn rule”](https://thehardestscience.com/2012/09/27/a-pottery-barn-rule-for-scientific-journals){:target="_blank"} means that journals agree to publish a direct replication of any study previously published in their journal. Other journals go beyond this, to agree to publish a replication of any study published in a major journal. In order to ensure that replications are assessed on the quality of their design rather than their results, a replication policy can include results masked review and/or be only for Registered Reports (see above).

[*Royal Society Open Science*](https://royalsocietypublishing.org/journal/rsos){:target="_blank"} offers a great example of a [replication policy](https://royalsocietypublishing.org/rsos/replication-studies){:target="_blank"} that adopts the (extended) pottery barn rule, and offers two tracks for review (results masked or Registered Report). See this [blog post](https://royalsociety.org/blog/2018/10/reproducibility-meets-accountability/){:target="_blank"} introducing their policy.

[*The Institute for Replication*](https://i4replication.org/index.html){:target="_blank"} has a compilation of sample replication reports from various Economics and Political Science journals, replication instructions, and educational material on replication and open science that can be shared with authors.

### Reporting guidelines {#reporting-guidelines}

Editors might consider suggesting or requiring that authors abide by discipline-specific reporting standards that include transparency requirements. For example, the [*American Psychological Association*](https://www.apa.org/){:target="_blank"} has a list of [Reporting Standards for Research in Psychology](https://doi.org/10.1037/0003-066X.63.9.839){:target="_blank"}. Specific reporting guidelines also exist for certain methodologies, e.g., the [Equator network](https://www.equator-network.org){:target="_blank"} has a list of [reporting guidelines for main study types in health research](https://www.equator-network.org/?post_type=eq_guidelines&eq_guidelines_study_design=systematic-reviews-and-meta-analyses&eq_guidelines_clinical_specialty=0&eq_guidelines_report_section=0&s=+){:target="_blank"}, including the [PRISMA guidelines for systematic reviews](https://www.equator-network.org/reporting-guidelines/prisma/){:target="_blank"}.

You may consider adding requirements for specific article types in your submission guidelines, e.g., see the [examples from JAMA](https://jamanetwork.com/journals/jama/pages/instructions-for-authors){:target="_blank"}.

#### Statistical guidelines {#statistical-guidelines}

One topic that editors might consider providing specific guidance on is statistics. [Hardwicke et al. (2022)](http://www.doi.org/10.31222/osf.io/q6ajt){:target="_blank"} found large gaps and inconsistent coverage in statistical guidance provided by top-ranked journals across scientific disciplines, and that the number of journals including statistical guidance for authors has changed little since the 90s ([Hardwicke & Goodman, 2020](https://doi.org/10.1371/journal.pone.0239598){:target="_blank"}). When creating statistical guidance, it is important to both consider what authors are being asked to report and how they are being asked to interpret statistics. [Fidler et al. (2004)](https://doi.org/10.1111/j.0963-7214.2004.01502008.x){:target="_blank"} found that even though instructing authors to provide confidence intervals in their papers increased this practice, authors very rarely used these to inform their discussion of the results. It is also important that editorial policies not selectively favor one side on controversies about statistical significance tests ([Mayo, 2021](https://doi.org/10.1111/cobi.13861){:target="_blank"}).

### Preprints {#preprints}

Preprints are a publicly available version of any type of scientific manuscript/research output preceding formal publication ([Parsons et al., 2022](https://forrt.org/glossary/%5Bhttps://doi.org/10.1038/s41562-021-01269-4){:target="_blank"}). Journals can support preprints by including links to preprint versions of the paper in the final published version, providing integrated workflows that enable authors to easily post a preprint when they submit their work to a journal (e.g., see [SciPost](https://scipost.org/){:target="_blank"}), supporting peer review organized around preprint platforms, or even making posting of preprints mandatory before review.